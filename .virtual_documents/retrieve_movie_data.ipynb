


# Dependencies
import requests
import time
from dotenv import load_dotenv
import os
import pandas as pd
import json


# Set environment variables from the .env in the local environment
load_dotenv()

nyt_api_key = os.getenv("NYT_API_KEY")
tmdb_api_key = os.getenv("TMDB_API_KEY")





# Set the base URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Filter for movie reviews with "love" in the headline
# section_name should be "Movies"
# type_of_material should be "Review"
filter_query = 'section_name:"Movies" AND type_of_material:"Review" AND headline:"love"'

# Use a sort filter, sort by newest
sort = "newest"

# Select the following fields to return:
# headline, web_url, snippet, source, keywords, pub_date, byline, word_count
field_list = "headline,web_url,snippet,source,keywords,pub_date,byline,word_count"

# Search for reviews published between a begin and end date
begin_date = "20130101"
end_date = "20230531"

# Build URL

query_url = (
    f"{url}"
    f"q=love&"  # General search term for keyword 'love'
    f"fq={filter_query}&"  # Filter query for section, type, and headline
    f"sort={sort}&"  # Sort by newest
    f"fl={field_list}&"  # Fields to return
    f"begin_date={begin_date}&"  # Start date for search
    f"end_date={end_date}&"  # End date for search
    f"api-key={nyt_api_key}"  # Your API key
)



# Create an empty list to store the reviews
reviews_list = []

# loop through pages 0-19
for page in range(20):
    # create query with a page number
    # API results show 10 articles at a time
    query_url_with_page = f"{query_url}&page={page}"
    
    # Make a "GET" request and retrieve the JSON
    response = requests.get(query_url_with_page)
    
    # Add a twelve second interval between queries to stay within API query limits
    time.sleep(12)
    
    # Try and save the reviews to the reviews_list
    try:
        # loop through the reviews["response"]["docs"] and append each review to the list
        reviews = response.json()
        if 'response' in reviews and 'docs' in reviews['response']:
            for review in reviews['response']['docs']:
                reviews_list.append(review)

            # Print the page that was just retrieved
            print(f"Page {page} retrieved successfully.")
        else:

            # Print the page number that had no results then break from the loop
            print(f"Page {page} had no results.")
            break
    # Handle any errors
    except Exception as e:
        print(f"Error on page {page}: {str(e)}")
        break


# Preview the first 5 results in JSON format
# Use json.dumps with argument indent=4 to format data

first_five_reviews = reviews_list[:5]

for i, review in enumerate(first_five_reviews):
    formatted_review = json.dumps(review, indent=4)
    print(f"Review {i+1}:")
    print(formatted_review)
    print("\n" + "="*80 + "\n")


# Convert reviews_list to a Pandas DataFrame using json_normalize()

df_reviews = pd.json_normalize(reviews_list)
df_reviews


# Extract the title from the "headline.main" column and
# save it to a new column "title"
# Title is between unicode characters \u2018 and \u2019. 
# End string should include " Review" to avoid cutting title early
df_reviews['title'] = df_reviews['headline.main'].apply(
    lambda st: st[st.find("\u2018")+1:st.find("\u2019 Review")+7]
)
df_reviews



# Extract 'name' and 'value' from items in "keywords" column
def extract_keywords(keyword_list):
    extracted_keywords = ""
    for item in keyword_list:
        # Extract 'name' and 'value'
        keyword = f"{item['name']}: {item['value']};" 
        # Append the keyword item to the extracted_keywords list
        extracted_keywords += keyword
    return extracted_keywords

# Fix the "keywords" column by converting cells from a list to a string
df_reviews['keywords'] = df_reviews['keywords'].apply(extract_keywords)
df_reviews


# Create a list from the "title" column using to_list()
# These titles will be used in the query for The Movie Database

df_reviews['title'] = df_reviews['headline.main'].apply(
    lambda st: st[st.find("\u2018")+1:st.find("\u2019 Review")]
)

titles = df_reviews['title'].to_list()
for title in titles:
    print(title)





# Prepare The Movie Database query
url = "https://api.themoviedb.org/3/search/movie?query="
tmdb_key_string = "&api_key=" + tmdb_api_key


# Create an empty list to store the results
tmdb_movies_list = []

# Create a request counter to sleep the requests after a multiple
# of 50 requests
request_counter = 1

# Loop through the titles
for title in titles:
    # Check if we need to sleep before making a request
    if request_counter % 50 == 0:
        print("Sleeping for a moment to respect API limits...")
        time.sleep(1)

    # Add 1 to the request counter
    request_counter += 1
    
    # Perform a "GET" request for The Movie Database
    search_url = f"{url}{title}{tmdb_key_string}"
    search_response = requests.get(search_url).json()


    # Include a try clause to search for the full movie details.
    # Use the except clause to print out a statement if a movie
    # is not found.
    try:
        # Get movie id
        movie_id = search_response['results'][0]['id']

        # Make a request for a the full movie details
        movie_url = f"https://api.themoviedb.org/3/movie/{movie_id}?api_key={tmdb_api_key}"
        movie_response = requests.get(movie_url).json()

        # Execute "GET" request with url
        movie_response = requests.get(movie_url).json()
        
        # Extract the genre names into a list
        genres = [genre['name'] for genre in movie_response.get('genres', [])]

        # Extract the spoken_languages' English name into a list
        spoken_languages = [language['english_name'] for language in movie_response.get('spoken_languages', [])]

        # Extract the production_countries' name into a list
        production_countries = [country['name'] for country in movie_response.get('production_countries', [])]
    
        # Add the relevant data to a dictionary and
        # append it to the tmdb_movies_list list
        movie_data = {
            'title': movie_response.get('title', ''),
            'original_title': movie_response.get('original_title', ''),
            'budget': movie_response.get('budget', 0),
            'original_language': movie_response.get('original_language', ''),
            'homepage': movie_response.get('homepage', ''),
            'overview': movie_response.get('overview', ''),
            'popularity': movie_response.get('popularity', 0),
            'runtime': movie_response.get('runtime', 0),
            'revenue': movie_response.get('revenue', 0),
            'release_date': movie_response.get('release_date', ''),
            'vote_average': movie_response.get('vote_average', 0),
            'vote_count': movie_response.get('vote_count', 0),
            'genres': genres,
            'spoken_languages': spoken_languages,
            'production_countries': production_countries
        }
        tmdb_movies_list.append(movie_data)
        
        # Print out the title that was found
        print(f"Found {movie_data['title']}")
    except (IndexError, KeyError):
         print(f"{title} not found.")


# Preview the first 5 results in JSON format
# Use json.dumps with argument indent=4 to format data
for i in range(min(5, len(tmdb_movies_list))):
    formatted_result = json.dumps(tmdb_movies_list[i], indent=4)
    print(formatted_result)
    print("\n")


# Convert the results to a DataFrame
tmdb_df = pd.DataFrame(tmdb_movies_list)
tmdb_df





# Remove any duplicate rows based on the 'title' column in both DataFrames
tmdb_df = tmdb_df.drop_duplicates(subset='title')
df_reviews = df_reviews.drop_duplicates(subset='title')

# Merge the New York Times reviews and TMDB DataFrames on title
merged_df = pd.merge(tmdb_df, df_reviews, on="title", how="inner")

# Adjust Pandas display options to show more columns
pd.set_option('display.max_columns', None)

merged_df


# Remove list brackets and quotation marks on the columns containing lists

# Create a list of the columns that need fixing
columns_to_fix = ['genres', 'spoken_languages', 'production_countries']

# Create a list of characters to remove
characters_to_remove = ["[", "]", "'"]

# Loop through the list of columns to fix
for column in columns_to_fix:
    # Convert the column to type 'str'
    merged_df[column] = merged_df[column].astype(str)
    
    # Loop through characters to remove
    for char in characters_to_remove:
        merged_df[column] = merged_df[column].str.replace(char, "")

# Display the fixed DataFrame
merged_df.head()


# Drop "byline.person" column
merged_df = merged_df.drop(columns=["byline.person"])


# Delete duplicate rows and reset index
merged_df = merged_df.drop_duplicates(subset='title')
merged_df = merged_df.reset_index(drop=True)
merged_df.head()


# Export data to CSV without the index
merged_df.to_csv("merged_data.csv", index=False)



